<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Meet Voice Bot Bridge</title>
    <style>
      :root {
        --bg: #0f172a;
        --surface: #1e293b;
        --text: #e2e8f0;
        --muted: #94a3b8;
        --ok: #16a34a;
        --warn: #ea580c;
      }
      body {
        margin: 0;
        font-family: "IBM Plex Sans", "Segoe UI", sans-serif;
        background: radial-gradient(circle at top right, #1d4ed8, var(--bg) 45%);
        color: var(--text);
      }
      .panel {
        max-width: 900px;
        margin: 24px auto;
        padding: 20px;
        background: color-mix(in srgb, var(--surface) 88%, black);
        border: 1px solid #334155;
        border-radius: 12px;
      }
      h1 {
        margin: 0 0 12px;
        font-size: 22px;
      }
      .status {
        margin: 0 0 16px;
        color: var(--muted);
      }
      .status strong {
        color: var(--ok);
      }
      .status strong.warn {
        color: var(--warn);
      }
      #log {
        margin: 0;
        padding: 12px;
        white-space: pre-wrap;
        font: 13px/1.4 ui-monospace, SFMono-Regular, Menlo, monospace;
        background: #020617;
        border-radius: 8px;
        min-height: 220px;
      }
    </style>
  </head>
  <body>
    <main class="panel">
      <h1>Meet Voice Bot Bridge</h1>
      <p class="status">
        Recognition:
        <strong id="recognitionState">starting...</strong>
      </p>
      <p class="status">
        Speech synthesis:
        <strong id="speechState">idle</strong>
      </p>
      <pre id="log"></pre>
    </main>

    <script>
      (() => {
        const state = {
          speaking: false,
          shouldListen: false,
          audioUnlocked: false,
          language: "en-US",
          silenceMs: 1000,
          turnSilenceMs: 700,
          ttsDuckLevel: 0.22,
          ttsDuckActive: false,
          ttsOutputDeviceId: "",
          ttsOutputDeviceLabel: "",
          ttsOutputConfigured: false,
          ttsOutputSupportLogged: false,
          currentAudioElement: null,
          stt: {
            active: false,
            stream: null,
            chunkMs: 1200,
            partialsEnabled: true,
            partialEmitMs: 240,
            lastPartialEmitAtMs: 0,
            deviceId: "",
            audioContext: null,
            sourceNode: null,
            processorNode: null,
            sinkNode: null,
            flushTimer: null,
            pcmFrames: [],
            sampleRate: 48000,
            flushInFlight: false,
            preferLoopback: true,
            signalPeak: 0,
            gateUntilMs: 0,
            minSignalPeak: 0.004,
            lastSilenceDropLogAtMs: 0,
            vadState: "idle",
            lastVoiceAtMs: 0,
            segmentStartedAtMs: 0,
            bufferedSamples: 0,
            vadThreshold: 0.015,
            hangoverMs: 700,
            segmentMinMs: 900,
            segmentMaxMs: 7000,
            bargeInMinMs: 220,
            vadStartedAtMs: 0,
            vadConfirmed: false,
            lastSegmentDropLogAtMs: 0,
            vadEventActive: false
          },
          currentPlayback: null,
          playbackQueue: [],
          playbackQueueWaiters: [],
          playbackLoopPromise: null,
          realtime: {
            active: false,
            starting: false,
            model: "",
            localStream: null,
            localTrack: null,
            peerConnection: null,
            dataChannel: null,
            responseInProgress: false,
            currentResponseId: "",
            speechActive: false,
            speechStartedAtMs: 0,
            bargeInMinMs: 220,
            userPartialByItem: new Map(),
            assistantTextByResponse: new Map(),
            audioElement: null
          }
        };

        let unlockAudioContext = null;
        const LOOPBACK_INPUT_REGEX =
          /(blackhole|loopback|soundflower|vb-?cable|cable output|monitor of|virtual|aggregate)/i;

        const recognitionStateEl = document.getElementById("recognitionState");
        const speechStateEl = document.getElementById("speechState");
        const logEl = document.getElementById("log");

        function setRecognitionState(text, warning = false) {
          recognitionStateEl.textContent = text;
          recognitionStateEl.className = warning ? "warn" : "";
        }

        function setSpeechState(text, warning = false) {
          speechStateEl.textContent = text;
          speechStateEl.className = warning ? "warn" : "";
        }

        function addLog(message) {
          const ts = new Date().toISOString();
          const line = `[${ts}] ${message}`;
          logEl.textContent = `${line}\n${logEl.textContent}`.slice(0, 10000);
          if (typeof window.notifyBridgeLog === "function") {
            Promise.resolve(window.notifyBridgeLog(message)).catch(() => {});
          }
        }

        function emitBridgeEvent(payload = {}) {
          if (!payload || typeof payload !== "object") {
            return;
          }
          if (typeof window.notifyBridgeEvent !== "function") {
            return;
          }
          Promise.resolve(
            window.notifyBridgeEvent({
              ts: Date.now(),
              source: "openai-stt",
              ...payload
            })
          ).catch(() => {});
        }

        function effectiveTtsVolume() {
          if (!state.ttsDuckActive) {
            return 1;
          }
          const duck = Number(state.ttsDuckLevel);
          if (!Number.isFinite(duck)) {
            return 0.22;
          }
          return Math.max(0, Math.min(1, duck));
        }

        function applyCurrentAudioVolume() {
          const volume = effectiveTtsVolume();
          const audioElements = [
            state.currentAudioElement,
            state.realtime?.audioElement || null
          ];
          for (const audio of audioElements) {
            if (!audio) {
              continue;
            }
            try {
              audio.volume = volume;
            } catch (_) {
              // Ignore runtime audio volume update failures.
            }
          }
        }

        function parseQuery() {
          const params = new URLSearchParams(window.location.search);
          state.language = params.get("lang") || state.language;
          state.silenceMs = Number(params.get("silenceMs")) || state.silenceMs;
          state.turnSilenceMs =
            Number(params.get("turnSilenceMs")) || state.turnSilenceMs;
          const duckLevel = Number(params.get("ttsDuckLevel"));
          if (Number.isFinite(duckLevel)) {
            state.ttsDuckLevel = Math.max(0, Math.min(1, duckLevel));
          }
          state.ttsOutputDeviceId =
            (params.get("ttsOutputDeviceId") || state.ttsOutputDeviceId || "").trim();
          state.ttsOutputDeviceLabel =
            (
              params.get("ttsOutputDeviceLabel") ||
              state.ttsOutputDeviceLabel ||
              ""
            ).trim();
        }

        function bytesToBase64(bytes) {
          let binary = "";
          const chunkSize = 0x8000;
          for (let index = 0; index < bytes.length; index += chunkSize) {
            const slice = bytes.subarray(index, index + chunkSize);
            binary += String.fromCharCode(...slice);
          }
          return btoa(binary);
        }

        function formatDeviceLabel(device, index) {
          const label = String(device?.label || "").trim() || `audioinput#${index + 1}`;
          const id = String(device?.deviceId || "").trim() || "unknown";
          return `${label} [${id}]`;
        }

        async function listAudioInputDevices() {
          try {
            const devices = await navigator.mediaDevices.enumerateDevices();
            return devices.filter((device) => device.kind === "audioinput");
          } catch (_) {
            return [];
          }
        }

        async function listAudioOutputDevices() {
          try {
            const devices = await navigator.mediaDevices.enumerateDevices();
            return devices.filter((device) => device.kind === "audiooutput");
          } catch (_) {
            return [];
          }
        }

        function findDeviceByLabel(devices, requestedLabel) {
          if (!requestedLabel) {
            return null;
          }
          const needle = requestedLabel.toLowerCase().trim();
          if (!needle) {
            return null;
          }
          return (
            devices.find((device) =>
              String(device.label || "")
                .toLowerCase()
                .includes(needle)
            ) || null
          );
        }

        async function applyTtsOutputDevice(audioEl) {
          if (!audioEl || typeof audioEl.setSinkId !== "function") {
            if (!state.ttsOutputSupportLogged) {
              addLog("tts output device routing is unavailable; using default output.");
              state.ttsOutputSupportLogged = true;
            }
            return;
          }

          let targetDeviceId = state.ttsOutputDeviceId;

          if (!targetDeviceId && state.ttsOutputDeviceLabel) {
            const outputs = await listAudioOutputDevices();
            const byLabel = findDeviceByLabel(outputs, state.ttsOutputDeviceLabel);
            if (byLabel?.deviceId) {
              targetDeviceId = byLabel.deviceId;
              state.ttsOutputDeviceId = byLabel.deviceId;
              addLog(
                `tts output selected by label: ${byLabel.label || "unknown"} [${byLabel.deviceId}]`
              );
            } else if (!state.ttsOutputConfigured) {
              const visibleOutputs = outputs.length
                ? outputs
                    .map(
                      (device, index) =>
                        `${String(device?.label || "").trim() || `audiooutput#${index + 1}`} [${String(device?.deviceId || "").trim() || "unknown"}]`
                    )
                    .join(" | ")
                : "none detected";
              addLog(`tts output devices: ${visibleOutputs}`);
              addLog(
                `tts output device label not found: "${state.ttsOutputDeviceLabel}" (using default output)`
              );
            }
          }

          if (!targetDeviceId) {
            return;
          }

          try {
            await audioEl.setSinkId(targetDeviceId);
            if (!state.ttsOutputConfigured) {
              addLog(`tts output routed to deviceId=${targetDeviceId}`);
              state.ttsOutputConfigured = true;
            }
          } catch (err) {
            if (!state.ttsOutputConfigured) {
              addLog(
                `tts output routing failed for deviceId=${targetDeviceId}: ${err?.message || err}`
              );
              state.ttsOutputConfigured = true;
            }
          }
        }

        async function prepareTtsOutput() {
          try {
            await unlockAudio();
            const probe = new Audio(
              "data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEAIlYAAESsAAACABAAZGF0YQAAAAA="
            );
            probe.preload = "auto";
            probe.muted = true;
            probe.volume = 0;
            await applyTtsOutputDevice(probe);
            return true;
          } catch (err) {
            addLog(`tts output prewarm failed: ${err?.message || err}`);
            return false;
          }
        }

        function findLoopbackDevice(devices) {
          return (
            devices.find((device) =>
              LOOPBACK_INPUT_REGEX.test(String(device.label || ""))
            ) || null
          );
        }

        async function ensureInputLabelsAvailable() {
          let inputs = await listAudioInputDevices();
          const hasLabels = inputs.some((device) => String(device.label || "").trim());
          if (hasLabels || inputs.length === 0) {
            return inputs;
          }

          // On some browsers labels are hidden until at least one media permission grant.
          try {
            const probe = await navigator.mediaDevices.getUserMedia({
              audio: true,
              video: false
            });
            for (const track of probe.getTracks()) {
              try {
                track.stop();
              } catch (_) {
                // Ignore track stop errors.
              }
            }
          } catch (_) {
            // Ignore permission probe failures.
          }

          inputs = await listAudioInputDevices();
          return inputs;
        }

        function float32ToInt16Pcm(floatArray) {
          const pcm = new Int16Array(floatArray.length);
          for (let index = 0; index < floatArray.length; index += 1) {
            const sample = Math.max(-1, Math.min(1, floatArray[index]));
            pcm[index] = sample < 0 ? sample * 0x8000 : sample * 0x7fff;
          }
          return pcm;
        }

        function concatInt16Chunks(chunks) {
          let totalLength = 0;
          for (const chunk of chunks) {
            totalLength += chunk.length;
          }

          const merged = new Int16Array(totalLength);
          let offset = 0;
          for (const chunk of chunks) {
            merged.set(chunk, offset);
            offset += chunk.length;
          }
          return merged;
        }

        function encodeWavPcm16(samples, sampleRate, channels = 1) {
          const bytesPerSample = 2;
          const dataByteLength = samples.length * bytesPerSample;
          const blockAlign = channels * bytesPerSample;
          const byteRate = sampleRate * blockAlign;
          const buffer = new ArrayBuffer(44 + dataByteLength);
          const view = new DataView(buffer);

          const writeAscii = (offset, value) => {
            for (let index = 0; index < value.length; index += 1) {
              view.setUint8(offset + index, value.charCodeAt(index));
            }
          };

          writeAscii(0, "RIFF");
          view.setUint32(4, 36 + dataByteLength, true);
          writeAscii(8, "WAVE");
          writeAscii(12, "fmt ");
          view.setUint32(16, 16, true);
          view.setUint16(20, 1, true);
          view.setUint16(22, channels, true);
          view.setUint32(24, sampleRate, true);
          view.setUint32(28, byteRate, true);
          view.setUint16(32, blockAlign, true);
          view.setUint16(34, 16, true);
          writeAscii(36, "data");
          view.setUint32(40, dataByteLength, true);

          let offset = 44;
          for (let index = 0; index < samples.length; index += 1) {
            view.setInt16(offset, samples[index], true);
            offset += 2;
          }

          return new Uint8Array(buffer);
        }

        function resetOpenAiSttSegmentState() {
          state.stt.pcmFrames = [];
          state.stt.bufferedSamples = 0;
          state.stt.signalPeak = 0;
          state.stt.vadState = "idle";
          state.stt.segmentStartedAtMs = 0;
          state.stt.lastVoiceAtMs = 0;
          state.stt.lastPartialEmitAtMs = 0;
          state.stt.vadStartedAtMs = 0;
          state.stt.vadConfirmed = false;
        }

        function emitVadStop(reason = "silence") {
          if (!state.stt.vadEventActive) {
            state.stt.vadConfirmed = false;
            state.stt.vadStartedAtMs = 0;
            return;
          }
          state.stt.vadEventActive = false;
          state.stt.vadConfirmed = false;
          state.stt.vadStartedAtMs = 0;
          addLog(`openai-stt vad.stop (reason=${reason})`);
          emitBridgeEvent({
            type: "vad.stop",
            reason
          });
        }

        function buildSegmentPayload(frames, bufferedSamples) {
          const merged = concatInt16Chunks(frames);
          if (!merged.length) {
            return null;
          }

          const durationMs = Math.max(
            0,
            Math.round(
              (bufferedSamples || merged.length) /
                Math.max(1, Number(state.stt.sampleRate || 48000)) *
                1000
            )
          );
          const wavBytes = encodeWavPcm16(merged, state.stt.sampleRate, 1);
          return {
            audioBase64: bytesToBase64(wavBytes),
            durationMs
          };
        }

        async function emitOpenAiSttPartial(reason = "interval") {
          if (
            !state.stt.active ||
            !state.stt.partialsEnabled ||
            state.stt.flushInFlight
          ) {
            return;
          }
          if (!Array.isArray(state.stt.pcmFrames) || state.stt.pcmFrames.length === 0) {
            return;
          }
          if (typeof window.notifyBridgeAudioChunk !== "function") {
            return;
          }

          const now = Date.now();
          const emitIntervalMs = Math.max(
            120,
            Number(state.stt.partialEmitMs || state.stt.chunkMs || 240)
          );
          if (now - Number(state.stt.lastPartialEmitAtMs || 0) < emitIntervalMs) {
            return;
          }

          const peak = Number(state.stt.signalPeak || 0);
          const minSignalPeak = Math.max(
            0,
            Math.min(1, Number(state.stt.minSignalPeak || 0))
          );
          if (peak < minSignalPeak) {
            return;
          }

          const payload = buildSegmentPayload(
            state.stt.pcmFrames,
            Number(state.stt.bufferedSamples || 0)
          );
          if (!payload || payload.durationMs < Math.max(120, emitIntervalMs - 80)) {
            return;
          }

          try {
            await window.notifyBridgeAudioChunk({
              audioBase64: payload.audioBase64,
              mimeType: "audio/wav",
              durationMs: payload.durationMs,
              isSegmentFinal: false,
              ts: now
            });
            state.stt.lastPartialEmitAtMs = now;
            addLog(
              `openai-stt partial sent (${payload.durationMs}ms, peak=${peak.toFixed(4)}, reason=${reason})`
            );
          } catch (err) {
            addLog(`openai-stt partial send failed: ${err?.message || err}`);
          }
        }

        async function flushOpenAiSttSegment(reason = "segment") {
          if (!state.stt.active || state.stt.flushInFlight) {
            return;
          }
          if (!Array.isArray(state.stt.pcmFrames) || state.stt.pcmFrames.length === 0) {
            resetOpenAiSttSegmentState();
            return;
          }
          if (typeof window.notifyBridgeAudioChunk !== "function") {
            resetOpenAiSttSegmentState();
            return;
          }

          const frames = state.stt.pcmFrames;
          const bufferedSamples = Number(state.stt.bufferedSamples || 0);
          const peak = Number(state.stt.signalPeak || 0);
          emitVadStop(reason);
          resetOpenAiSttSegmentState();
          const payload = buildSegmentPayload(frames, bufferedSamples);
          if (!payload) {
            return;
          }
          const durationMs = payload.durationMs;

          const minSignalPeak = Math.max(
            0,
            Math.min(1, Number(state.stt.minSignalPeak || 0))
          );
          if (peak < minSignalPeak) {
            const now = Date.now();
            if (now - Number(state.stt.lastSilenceDropLogAtMs || 0) > 2500) {
              addLog(
                `openai-stt segment dropped as silence (peak=${peak.toFixed(4)}, threshold=${minSignalPeak.toFixed(4)})`
              );
              state.stt.lastSilenceDropLogAtMs = now;
            }
            return;
          }

          const segmentMinMs = Math.max(
            120,
            Number(state.stt.segmentMinMs || 900)
          );
          if (durationMs < segmentMinMs) {
            const now = Date.now();
            if (now - Number(state.stt.lastSegmentDropLogAtMs || 0) > 2500) {
              addLog(
                `openai-stt segment dropped as too short (${durationMs}ms < ${segmentMinMs}ms)`
              );
              state.stt.lastSegmentDropLogAtMs = now;
            }
            return;
          }

          state.stt.flushInFlight = true;
          try {
            await window.notifyBridgeAudioChunk({
              audioBase64: payload.audioBase64,
              mimeType: "audio/wav",
              durationMs,
              isSegmentFinal: true,
              ts: Date.now()
            });
            addLog(
              `openai-stt segment sent (${durationMs}ms, peak=${peak.toFixed(4)}, reason=${reason})`
            );
          } catch (err) {
            addLog(`openai-stt segment send failed: ${err?.message || err}`);
          } finally {
            state.stt.flushInFlight = false;
          }
        }

        async function unlockAudio() {
          if (state.audioUnlocked) {
            return true;
          }

          try {
            const AudioContextCtor =
              window.AudioContext || window.webkitAudioContext;
            if (AudioContextCtor) {
              if (!unlockAudioContext) {
                unlockAudioContext = new AudioContextCtor();
              }

              if (unlockAudioContext.state !== "running") {
                await unlockAudioContext.resume();
              }

              const oscillator = unlockAudioContext.createOscillator();
              const gain = unlockAudioContext.createGain();
              gain.gain.value = 0.0001;
              oscillator.connect(gain);
              gain.connect(unlockAudioContext.destination);
              const startedAt = unlockAudioContext.currentTime;
              oscillator.start(startedAt);
              oscillator.stop(startedAt + 0.02);
            }

            const probe = new Audio(
              "data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEAIlYAAESsAAACABAAZGF0YQAAAAA="
            );
            probe.muted = true;
            probe.volume = 0;
            try {
              await probe.play();
              probe.pause();
            } catch (_) {
              // Ignore playback probe errors.
            }

            state.audioUnlocked = true;
            addLog("audio unlocked");
            return true;
          } catch (err) {
            addLog(`audio unlock failed: ${err?.message || err}`);
            return false;
          }
        }

        function attachPlaybackStop(controller, stopHandler) {
          if (!controller || typeof stopHandler !== "function") {
            return;
          }
          controller.stopHandlers.push(stopHandler);
        }

        function notifyPlaybackQueueWaiters() {
          if (!Array.isArray(state.playbackQueueWaiters) || !state.playbackQueueWaiters.length) {
            return;
          }
          const waiters = state.playbackQueueWaiters.splice(0);
          for (const notify of waiters) {
            try {
              notify(true);
            } catch (_) {
              // Ignore waiter notification errors.
            }
          }
        }

        function waitForPlaybackQueueSignal(timeoutMs = 45) {
          if (Array.isArray(state.playbackQueue) && state.playbackQueue.length > 0) {
            return Promise.resolve(true);
          }

          return new Promise((resolve) => {
            let settled = false;
            const complete = (value) => {
              if (settled) {
                return;
              }
              settled = true;
              resolve(Boolean(value));
            };
            const timer = setTimeout(() => complete(false), Math.max(1, timeoutMs));
            state.playbackQueueWaiters.push(() => {
              clearTimeout(timer);
              complete(true);
            });
          });
        }

        function resolveAndClearPlaybackQueue(result = false) {
          if (!Array.isArray(state.playbackQueue) || state.playbackQueue.length === 0) {
            return;
          }
          const pending = state.playbackQueue.splice(0);
          for (const item of pending) {
            try {
              item?.resolve?.(Boolean(result));
            } catch (_) {
              // Ignore queue item resolve errors.
            }
          }
        }

        async function playAudioChunkWithPlayback({
          audioBase64,
          mimeType = "audio/mpeg",
          playback
        } = {}) {
          const source = `data:${mimeType};base64,${audioBase64}`;
          const audio = new Audio(source);
          audio.preload = "auto";
          audio.volume = effectiveTtsVolume();
          await applyTtsOutputDevice(audio);
          state.currentAudioElement = audio;

          return new Promise((resolve) => {
            let resolved = false;
            const done = (ok, errorMessage = "") => {
              if (resolved) {
                return;
              }
              resolved = true;
              if (state.currentAudioElement === audio) {
                state.currentAudioElement = null;
              }
              resolve({
                ok: Boolean(ok),
                errorMessage: String(errorMessage || "")
              });
            };

            attachPlaybackStop(playback, () => {
              try {
                audio.pause();
                audio.currentTime = 0;
                audio.src = "";
                audio.load();
              } catch (_) {
                // Ignore audio teardown errors.
              }
              done(false, "playback interrupted");
            });

            audio.onended = () => done(true);
            audio.onerror = () => done(false, "audio element error");

            const playPromise = audio.play();
            if (playPromise && typeof playPromise.catch === "function") {
              playPromise.catch((err) => {
                done(
                  false,
                  err?.message || (err instanceof Error ? err.message : String(err))
                );
              });
            }
          });
        }

        async function ensureStreamingPlaybackLoop() {
          if (state.playbackLoopPromise) {
            return state.playbackLoopPromise;
          }

          state.playbackLoopPromise = withSpeechLock(async (playback) => {
            while (!playback.stopped) {
              if (!Array.isArray(state.playbackQueue) || state.playbackQueue.length === 0) {
                const hasMore = await waitForPlaybackQueueSignal(45);
                if (!hasMore || playback.stopped) {
                  break;
                }
              }

              const nextItem = state.playbackQueue.shift();
              if (!nextItem) {
                continue;
              }

              const result = await playAudioChunkWithPlayback({
                audioBase64: nextItem.audioBase64,
                mimeType: nextItem.mimeType,
                playback
              });

              if (result.ok) {
                addLog(`synthesized(openai): ${nextItem.spokenText || "<audio>"}`);
              } else if (result.errorMessage && result.errorMessage !== "playback interrupted") {
                addLog(`openai-audio play failed: ${result.errorMessage}`);
              }
              nextItem.resolve(result.ok);
            }

            if (playback.stopped) {
              resolveAndClearPlaybackQueue(false);
            }
          })
            .catch((err) => {
              addLog(`stream playback loop failed: ${err?.message || err}`);
              resolveAndClearPlaybackQueue(false);
            })
            .finally(() => {
              state.playbackLoopPromise = null;
              notifyPlaybackQueueWaiters();
            });

          return state.playbackLoopPromise;
        }

        function createRealtimeEventId(prefix = "evt") {
          return `${prefix}_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
        }

        function parseLanguageTag(value) {
          const normalized = String(value || "").trim().toLowerCase();
          if (!normalized) {
            return "";
          }
          const [primary] = normalized.split("-");
          if (!primary || primary.length < 2 || primary.length > 3) {
            return "";
          }
          return primary;
        }

        function buildRealtimeTurnDetection(options = {}) {
          const type = String(options.turnDetection || "manual")
            .trim()
            .toLowerCase();
          if (type === "server_vad") {
            return {
              type: "server_vad",
              threshold: Number.isFinite(Number(options.vadThreshold))
                ? Math.max(0, Math.min(1, Number(options.vadThreshold)))
                : 0.45,
              silence_duration_ms: Number.isFinite(Number(options.vadSilenceMs))
                ? Math.max(120, Math.min(2000, Math.trunc(Number(options.vadSilenceMs))))
                : 280,
              prefix_padding_ms: Number.isFinite(Number(options.vadPrefixPaddingMs))
                ? Math.max(0, Math.min(1000, Math.trunc(Number(options.vadPrefixPaddingMs))))
                : 180,
              create_response: true,
              interrupt_response: options.interruptResponseOnTurn !== false
            };
          }
          if (type === "semantic_vad") {
            const eagerness = String(options.turnDetectionEagerness || "auto")
              .trim()
              .toLowerCase();
            return {
              type: "semantic_vad",
              eagerness: ["low", "medium", "high", "auto"].includes(eagerness)
                ? eagerness
                : "auto",
              create_response: true,
              interrupt_response: options.interruptResponseOnTurn !== false
            };
          }
          return null;
        }

        function emitRealtimePlaybackStart({ responseId = "" } = {}) {
          if (state.realtime.responseInProgress) {
            return;
          }
          state.realtime.responseInProgress = true;
          state.realtime.currentResponseId = String(responseId || "").trim();
          addLog("tts playback start (gateMs=0, mode=realtime)");
          emitBridgeEvent({
            source: "bridge-tts",
            type: "tts.playback.start",
            gateMs: 0
          });
        }

        function emitRealtimePlaybackEnd({ interrupted = false } = {}) {
          if (!state.realtime.responseInProgress) {
            return;
          }
          state.realtime.responseInProgress = false;
          state.realtime.currentResponseId = "";
          addLog(
            `tts playback end (state=${interrupted ? "interrupted" : "completed"}, gateMs=0, mode=realtime)`
          );
          emitBridgeEvent({
            source: "bridge-tts",
            type: "tts.playback.end",
            gateMs: 0,
            interrupted: Boolean(interrupted)
          });
        }

        function sendRealtimeEvent(event = {}) {
          const channel = state.realtime.dataChannel;
          if (!channel || channel.readyState !== "open") {
            return false;
          }
          try {
            channel.send(JSON.stringify(event));
            return true;
          } catch (_) {
            return false;
          }
        }

        function handleRealtimeServerEvent(event = {}) {
          const type = String(event?.type || "").trim().toLowerCase();
          if (!type) {
            return;
          }

          if (type === "session.created") {
            const model = String(event?.session?.model || state.realtime.model || "").trim();
            addLog(`realtime connected (model=${model || "unknown"}).`);
            return;
          }

          if (type === "session.updated") {
            const td = String(event?.session?.turn_detection?.type || "manual").trim();
            addLog(`realtime session updated (turn_detection=${td || "manual"}).`);
            return;
          }

          if (type === "input_audio_buffer.speech_started") {
            state.realtime.speechActive = true;
            state.realtime.speechStartedAtMs = Date.now();
            emitBridgeEvent({
              source: "openai-realtime",
              type: "vad.start",
              reason: "realtime-speech-started"
            });
            return;
          }

          if (type === "input_audio_buffer.speech_stopped") {
            const speechMs = Math.max(
              0,
              Date.now() - Number(state.realtime.speechStartedAtMs || 0)
            );
            if (
              state.realtime.speechActive &&
              speechMs >= Math.max(80, Number(state.realtime.bargeInMinMs || 220))
            ) {
              emitBridgeEvent({
                source: "openai-realtime",
                type: "vad.confirmed",
                reason: "realtime-speech-stopped",
                speechMs
              });
            }
            state.realtime.speechActive = false;
            state.realtime.speechStartedAtMs = 0;
            emitBridgeEvent({
              source: "openai-realtime",
              type: "vad.stop",
              reason: "realtime-speech-stopped"
            });
            return;
          }

          if (type === "conversation.item.input_audio_transcription.delta") {
            const itemId = String(event?.item_id || "").trim();
            const delta = String(event?.delta || "").trim();
            if (!itemId || !delta) {
              return;
            }
            const previous = state.realtime.userPartialByItem.get(itemId) || "";
            const next = `${previous} ${delta}`.replace(/\s+/g, " ").trim();
            if (!next) {
              return;
            }
            state.realtime.userPartialByItem.set(itemId, next);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "transcript.partial",
              text: next,
              turnId: itemId
            });
            return;
          }

          if (type === "conversation.item.input_audio_transcription.completed") {
            const itemId = String(event?.item_id || "").trim();
            const transcript = String(event?.transcript || "").trim();
            if (itemId) {
              state.realtime.userPartialByItem.delete(itemId);
            }
            if (!transcript) {
              return;
            }
            emitBridgeEvent({
              source: "openai-realtime",
              type: "transcript.final",
              text: transcript,
              isFinal: true,
              turnId: itemId || undefined
            });
            emitBridgeEvent({
              source: "openai-realtime",
              type: "turn.final",
              text: transcript,
              isFinal: true,
              turnId: itemId || undefined
            });
            return;
          }

          if (type === "response.created") {
            const responseId = String(event?.response?.id || "").trim();
            emitRealtimePlaybackStart({ responseId });
            emitBridgeEvent({
              source: "openai-realtime",
              type: "assistant.response.started",
              responseId: responseId || undefined
            });
            return;
          }

          if (type === "response.audio_transcript.delta") {
            const responseId = String(event?.response_id || "").trim();
            const delta = String(event?.delta || "").trim();
            if (!responseId || !delta) {
              return;
            }
            const previous = state.realtime.assistantTextByResponse.get(responseId) || "";
            const next = `${previous} ${delta}`.replace(/\s+/g, " ").trim();
            if (!next) {
              return;
            }
            state.realtime.assistantTextByResponse.set(responseId, next);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "assistant.text.partial",
              responseId,
              text: next
            });
            return;
          }

          if (type === "response.audio_transcript.done") {
            const responseId = String(event?.response_id || "").trim();
            const transcript = String(event?.transcript || "").trim();
            if (!responseId || !transcript) {
              return;
            }
            state.realtime.assistantTextByResponse.set(responseId, transcript);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "assistant.text.final",
              responseId,
              text: transcript,
              isFinal: true
            });
            return;
          }

          if (type === "response.text.done") {
            const responseId = String(event?.response_id || "").trim();
            const text = String(event?.text || "").trim();
            if (!responseId || !text) {
              return;
            }
            if (state.realtime.assistantTextByResponse.has(responseId)) {
              return;
            }
            state.realtime.assistantTextByResponse.set(responseId, text);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "assistant.text.final",
              responseId,
              text,
              isFinal: true
            });
            return;
          }

          if (type === "response.done") {
            const responseId = String(
              event?.response?.id || state.realtime.currentResponseId || ""
            ).trim();
            const status = String(event?.response?.status || "unknown").trim() || "unknown";
            const reason = String(event?.response?.status_details?.reason || "").trim();
            emitRealtimePlaybackEnd({
              interrupted: status !== "completed" && status !== "incomplete"
            });
            emitBridgeEvent({
              source: "openai-realtime",
              type: "assistant.response.done",
              responseId: responseId || undefined,
              status,
              reason: reason || undefined
            });
            return;
          }

          if (type === "error") {
            const message =
              String(event?.error?.message || event?.message || "Realtime transport error.")
                .replace(/\s+/g, " ")
                .trim() || "Realtime transport error.";
            addLog(`realtime error: ${message}`);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "realtime.error",
              reason: message
            });
          }
        }

        async function createRealtimeEphemeralSession(options = {}) {
          const response = await fetch("/realtime/session", {
            method: "POST",
            headers: {
              "Content-Type": "application/json"
            },
            body: JSON.stringify({
              model: options.model,
              language: options.language,
              instructions: options.instructions,
              voice: options.voice,
              temperature: options.temperature,
              inputTranscriptionModel: options.inputTranscriptionModel,
              turnDetection: options.turnDetection,
              turnDetectionEagerness: options.turnDetectionEagerness,
              vadThreshold: options.vadThreshold,
              vadSilenceMs: options.vadSilenceMs,
              vadPrefixPaddingMs: options.vadPrefixPaddingMs,
              interruptResponseOnTurn: options.interruptResponseOnTurn
            })
          });
          const payload = await response.json().catch(() => ({}));
          if (!response.ok) {
            throw new Error(
              String(payload?.error?.message || "Failed to create realtime session.")
            );
          }
          const clientSecret = String(payload?.clientSecret || "").trim();
          if (!clientSecret) {
            throw new Error("Realtime session token is missing in bridge response.");
          }
          return {
            clientSecret,
            session: payload?.session || {}
          };
        }

        async function resolveRealtimeInputStream(options = {}) {
          const requestedDeviceId = String(options.inputDeviceId || "").trim();
          const requestedDeviceLabel = String(options.inputDeviceLabel || "").trim();
          const preferLoopbackOption = options.inputPreferLoopback;
          const strictRequestedInput = Boolean(
            requestedDeviceId || requestedDeviceLabel
          );

          const audioConstraints = {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            channelCount: 1
          };

          const devices = await ensureInputLabelsAvailable();
          if (devices.length > 0) {
            addLog(
              `realtime audio inputs: ${devices
                .map((device, index) => formatDeviceLabel(device, index))
                .join(" | ")}`
            );
          } else {
            addLog("realtime audio inputs: none detected");
          }

          let selectedDeviceId = requestedDeviceId;
          let selectedDeviceLabel = "";

          if (selectedDeviceId) {
            const byId =
              devices.find(
                (device) =>
                  String(device?.deviceId || "").trim() === selectedDeviceId
              ) || null;
            if (byId?.label) {
              selectedDeviceLabel = byId.label;
            } else if (strictRequestedInput) {
              throw new Error(
                `Realtime input deviceId is not available: ${selectedDeviceId}`
              );
            }
          }

          if (!selectedDeviceId && requestedDeviceLabel) {
            const byLabel = findDeviceByLabel(devices, requestedDeviceLabel);
            if (byLabel?.deviceId) {
              selectedDeviceId = byLabel.deviceId;
              selectedDeviceLabel = byLabel.label || "";
            } else {
              throw new Error(
                `Realtime input device label not found: "${requestedDeviceLabel}"`
              );
            }
          } else if (!selectedDeviceId) {
            const preferLoopback =
              typeof preferLoopbackOption === "boolean"
                ? preferLoopbackOption
                : Boolean(state.stt.preferLoopback);
            if (preferLoopback) {
              const loopback = findLoopbackDevice(devices);
              if (loopback?.deviceId) {
                selectedDeviceId = loopback.deviceId;
                selectedDeviceLabel = loopback.label || "";
              }
            }
          }

          if (selectedDeviceId) {
            audioConstraints.deviceId = { exact: selectedDeviceId };
          }

          const requestStream = async (constraints) =>
            navigator.mediaDevices.getUserMedia({
              audio: constraints,
              video: false
            });

          let stream;
          try {
            stream = await requestStream(audioConstraints);
          } catch (err) {
            const errorName = String(err?.name || "");
            const isOverconstrained =
              errorName === "OverconstrainedError" ||
              errorName === "ConstraintNotSatisfiedError";
            if (!isOverconstrained) {
              throw err;
            }

            const relaxedWithDevice = selectedDeviceId
              ? { deviceId: { exact: selectedDeviceId } }
              : true;
            try {
              stream = await requestStream(relaxedWithDevice);
            } catch (relaxedErr) {
              const relaxedName = String(relaxedErr?.name || "");
              const relaxedOverconstrained =
                relaxedName === "OverconstrainedError" ||
                relaxedName === "ConstraintNotSatisfiedError";
              if (strictRequestedInput) {
                throw new Error(
                  `Realtime input device became unavailable: ${
                    relaxedErr?.message || relaxedErr
                  }`
                );
              }
              if (!selectedDeviceId || !relaxedOverconstrained) {
                throw relaxedErr;
              }
              stream = await requestStream(true);
              selectedDeviceId = "";
              selectedDeviceLabel = "";
            }
          }

          addLog(
            `realtime selected input: ${
              selectedDeviceLabel || "default input"
            } [${selectedDeviceId || "default"}]`
          );

          return {
            stream,
            selectedDeviceId,
            selectedDeviceLabel
          };
        }

        async function waitForDataChannelOpen(channel, timeoutMs = 8000) {
          if (!channel) {
            throw new Error("Realtime data channel is not initialized.");
          }
          if (channel.readyState === "open") {
            return true;
          }

          await new Promise((resolve, reject) => {
            let settled = false;
            const finish = (fn) => {
              if (settled) {
                return;
              }
              settled = true;
              cleanup();
              fn();
            };

            const cleanup = () => {
              clearTimeout(timer);
              channel.removeEventListener("open", onOpen);
              channel.removeEventListener("error", onError);
              channel.removeEventListener("close", onClose);
            };

            const onOpen = () => finish(() => resolve(true));
            const onError = () => finish(() => reject(new Error("Realtime data channel error.")));
            const onClose = () =>
              finish(() => reject(new Error("Realtime data channel closed before opening.")));
            const timer = setTimeout(() => {
              finish(() =>
                reject(new Error(`Realtime data channel open timed out after ${timeoutMs}ms.`))
              );
            }, Math.max(1000, Math.trunc(Number(timeoutMs) || 8000)));

            channel.addEventListener("open", onOpen);
            channel.addEventListener("error", onError);
            channel.addEventListener("close", onClose);
          });

          return true;
        }

        async function stopRealtime(options = {}) {
          const reason = String(options.reason || "stop").trim() || "stop";
          const realtime = state.realtime;
          if (!realtime.active && !realtime.starting) {
            return false;
          }

          realtime.starting = false;
          realtime.active = false;
          if (realtime.responseInProgress) {
            emitRealtimePlaybackEnd({ interrupted: true });
          }
          realtime.responseInProgress = false;
          realtime.currentResponseId = "";
          realtime.speechActive = false;
          realtime.speechStartedAtMs = 0;
          realtime.userPartialByItem.clear();
          realtime.assistantTextByResponse.clear();

          if (realtime.dataChannel) {
            try {
              realtime.dataChannel.close();
            } catch (_) {
              // Ignore data channel close errors.
            }
          }
          if (realtime.peerConnection) {
            try {
              realtime.peerConnection.close();
            } catch (_) {
              // Ignore peer connection close errors.
            }
          }
          if (realtime.localStream) {
            for (const track of realtime.localStream.getTracks()) {
              try {
                track.stop();
              } catch (_) {
                // Ignore local track stop errors.
              }
            }
          }
          if (realtime.audioElement) {
            try {
              realtime.audioElement.pause();
              realtime.audioElement.srcObject = null;
            } catch (_) {
              // Ignore audio element cleanup errors.
            }
          }

          realtime.localStream = null;
          realtime.localTrack = null;
          realtime.dataChannel = null;
          realtime.peerConnection = null;
          realtime.audioElement = null;
          realtime.model = "";
          addLog(`realtime stopped (reason=${reason})`);
          return true;
        }

        async function startRealtime(options = {}) {
          const realtime = state.realtime;
          if (realtime.active) {
            return true;
          }
          if (realtime.starting) {
            return false;
          }

          realtime.starting = true;
          realtime.bargeInMinMs = Number.isFinite(Number(options.bargeInMinMs))
            ? Math.max(80, Math.min(5000, Math.trunc(Number(options.bargeInMinMs))))
            : realtime.bargeInMinMs;

          try {
            await stopRealtime({ reason: "restart-before-start" }).catch(() => {});
            if (state.stt.active) {
              stopOpenAiStt({ preserveShouldListen: true });
            }
            await unlockAudio();

            const connectTimeoutMs = Number.isFinite(Number(options.connectTimeoutMs))
              ? Math.max(1000, Math.min(30000, Math.trunc(Number(options.connectTimeoutMs))))
              : 8000;
            const model = String(options.model || "gpt-4o-mini-realtime-preview-2024-12-17")
              .trim();
            const requestedTurnDetection = String(
              options.turnDetection || "semantic_vad"
            )
              .trim()
              .toLowerCase();
            const effectiveTurnDetection =
              requestedTurnDetection === "manual"
                ? "semantic_vad"
                : requestedTurnDetection;
            if (effectiveTurnDetection !== requestedTurnDetection) {
              addLog(
                "realtime turn_detection=manual disables live audio turns; using semantic_vad."
              );
            }

            const token = await createRealtimeEphemeralSession({
              model,
              language: parseLanguageTag(options.language),
              instructions: String(options.instructions || ""),
              voice: String(options.voice || "alloy"),
              temperature: Number(options.temperature),
              inputTranscriptionModel: String(
                options.inputTranscriptionModel || "gpt-4o-mini-transcribe"
              ),
              turnDetection: effectiveTurnDetection,
              turnDetectionEagerness: String(options.turnDetectionEagerness || "auto"),
              vadThreshold: Number(options.vadThreshold),
              vadSilenceMs: Number(options.vadSilenceMs),
              vadPrefixPaddingMs: Number(options.vadPrefixPaddingMs),
              interruptResponseOnTurn: options.interruptResponseOnTurn !== false
            });

            const { stream } = await resolveRealtimeInputStream(options);
            const connection = new RTCPeerConnection();
            const dataChannel = connection.createDataChannel("oai-events");

            const realtimeAudio = new Audio();
            realtimeAudio.autoplay = true;
            realtimeAudio.playsInline = true;
            realtimeAudio.volume = effectiveTtsVolume();
            await applyTtsOutputDevice(realtimeAudio);
            realtime.audioElement = realtimeAudio;

            connection.ontrack = (event) => {
              const [streamFromTrack] = event.streams || [];
              if (streamFromTrack) {
                realtimeAudio.srcObject = streamFromTrack;
              }
            };

            connection.onconnectionstatechange = () => {
              const stateName = String(connection.connectionState || "unknown");
              if (stateName === "failed" || stateName === "disconnected") {
                addLog(`realtime peer connection state: ${stateName}`);
                emitBridgeEvent({
                  source: "openai-realtime",
                  type: "realtime.error",
                  reason: `peer connection ${stateName}`
                });
              }
            };

            dataChannel.addEventListener("message", (messageEvent) => {
              try {
                const parsed = JSON.parse(String(messageEvent?.data || "{}"));
                handleRealtimeServerEvent(parsed);
              } catch (_) {
                // Ignore malformed realtime events.
              }
            });

            for (const track of stream.getAudioTracks()) {
              connection.addTrack(track, stream);
              realtime.localTrack = track;
            }

            const offer = await connection.createOffer({
              offerToReceiveAudio: true
            });
            await connection.setLocalDescription(offer);

            const sdpResponse = await fetch(
              `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`,
              {
                method: "POST",
                headers: {
                  Authorization: `Bearer ${token.clientSecret}`,
                  "Content-Type": "application/sdp"
                },
                body: String(offer.sdp || "")
              }
            );
            const answerSdp = await sdpResponse.text();
            if (!sdpResponse.ok) {
              throw new Error(
                `Realtime SDP exchange failed (${sdpResponse.status}): ${answerSdp}`
              );
            }

            await connection.setRemoteDescription({
              type: "answer",
              sdp: answerSdp
            });
            await waitForDataChannelOpen(dataChannel, connectTimeoutMs);

            realtime.model = model;
            realtime.localStream = stream;
            realtime.peerConnection = connection;
            realtime.dataChannel = dataChannel;
            realtime.active = true;

            sendRealtimeEvent({
              type: "session.update",
              event_id: createRealtimeEventId("sess_update"),
              session: {
                modalities: ["audio", "text"],
                instructions: String(options.instructions || ""),
                voice: String(options.voice || "alloy"),
                temperature: Number.isFinite(Number(options.temperature))
                  ? Number(options.temperature)
                  : 0.8,
                input_audio_transcription: {
                  model: String(
                    options.inputTranscriptionModel || "gpt-4o-mini-transcribe"
                  ),
                  language: parseLanguageTag(options.language) || undefined
                },
                turn_detection: buildRealtimeTurnDetection({
                  ...options,
                  turnDetection: effectiveTurnDetection
                })
              }
            });

            addLog(
              `realtime started (transport=webrtc, model=${model}, turnDetection=${String(
                effectiveTurnDetection
              )})`
            );
            return true;
          } catch (err) {
            await stopRealtime({ reason: "start-failed" }).catch(() => {});
            const message = String(err?.message || err || "Realtime start failed");
            addLog(`realtime start failed: ${message}`);
            emitBridgeEvent({
              source: "openai-realtime",
              type: "realtime.error",
              reason: message
            });
            return false;
          } finally {
            realtime.starting = false;
          }
        }

        async function realtimeCreateTextTurn(payload = {}) {
          if (!state.realtime.active || !state.realtime.dataChannel) {
            return false;
          }
          const text = String(payload.text || "").trim();
          if (!text) {
            return false;
          }
          const roleRaw = String(payload.role || "user").trim().toLowerCase();
          const role = roleRaw === "system" ? "system" : "user";
          const createResponse = payload.createResponse !== false;

          const created = sendRealtimeEvent({
            type: "conversation.item.create",
            event_id: createRealtimeEventId("item_create"),
            item: {
              type: "message",
              role,
              content: [
                {
                  type: "input_text",
                  text
                }
              ]
            }
          });
          if (!created) {
            return false;
          }

          if (createResponse) {
            sendRealtimeEvent({
              type: "response.create",
              event_id: createRealtimeEventId("resp_create"),
              response: {
                modalities: ["audio", "text"]
              }
            });
          }

          addLog(
            `realtime text turn created (role=${role}, chars=${text.length}, createResponse=${createResponse})`
          );
          return true;
        }

        async function realtimeAppendSystemContext(note = "") {
          const text = String(note || "").trim();
          if (!text) {
            return false;
          }
          return realtimeCreateTextTurn({
            role: "system",
            text,
            createResponse: false
          });
        }

        async function realtimeInterrupt(options = {}) {
          if (!state.realtime.active || !state.realtime.dataChannel) {
            return false;
          }
          sendRealtimeEvent({
            type: "response.cancel",
            event_id: createRealtimeEventId("resp_cancel")
          });
          sendRealtimeEvent({
            type: "output_audio_buffer.clear",
            event_id: createRealtimeEventId("out_clear")
          });
          if (options?.clearInputBuffer) {
            sendRealtimeEvent({
              type: "input_audio_buffer.clear",
              event_id: createRealtimeEventId("in_clear")
            });
          }
          emitRealtimePlaybackEnd({ interrupted: true });
          addLog(
            `realtime response interrupted (${String(options?.reason || "unknown").trim() || "unknown"}).`
          );
          return true;
        }

        function stopSpeaking(options = {}) {
          const flush = options?.flush !== false;
          const resumeGateMs = Number.isFinite(Number(options?.resumeGateMs))
            ? Math.max(0, Math.trunc(Number(options.resumeGateMs)))
            : Math.min(80, Math.max(0, Number(state.silenceMs || 0)));
          const queuedCount = Array.isArray(state.playbackQueue)
            ? state.playbackQueue.length
            : 0;
          if (queuedCount > 0) {
            resolveAndClearPlaybackQueue(false);
            notifyPlaybackQueueWaiters();
          }
          const playback = state.currentPlayback;
          if (!playback || playback.stopped) {
            if (queuedCount > 0) {
              addLog(`stopSpeaking cleared queued playback (${queuedCount} chunks)`);
            } else {
              addLog("stopSpeaking ignored: no active playback");
            }
            if (flush) {
              state.stt.gateUntilMs = Date.now() + resumeGateMs;
            }
            return queuedCount > 0;
          }

          playback.stopped = true;
          for (const stopHandler of playback.stopHandlers) {
            try {
              stopHandler("manual-stop");
            } catch (_) {
              // Ignore individual stop handler errors.
            }
          }
          playback.stopHandlers = [];
          if (flush) {
            state.stt.gateUntilMs = Date.now() + resumeGateMs;
          }
          addLog("stopSpeaking applied: active playback interrupted");
          return true;
        }

        function setTtsDucking(options = {}) {
          const active =
            typeof options.active === "boolean"
              ? options.active
              : Boolean(options);
          if (Number.isFinite(options.level)) {
            state.ttsDuckLevel = Math.max(0, Math.min(1, Number(options.level)));
          }
          state.ttsDuckActive = active;
          applyCurrentAudioVolume();
          addLog(
            `tts ducking ${active ? "enabled" : "disabled"} (level=${effectiveTtsVolume().toFixed(2)})`
          );
          return true;
        }

        function stopOpenAiStt(options = {}) {
          const preserveShouldListen = Boolean(options.preserveShouldListen);
          if (!preserveShouldListen) {
            state.shouldListen = false;
          }

          const stream = state.stt.stream;
          const audioContext = state.stt.audioContext;
          const sourceNode = state.stt.sourceNode;
          const processorNode = state.stt.processorNode;
          const sinkNode = state.stt.sinkNode;
          const flushTimer = state.stt.flushTimer;

          emitVadStop("stt-stop");
          state.stt.active = false;
          state.stt.stream = null;
          state.stt.audioContext = null;
          state.stt.sourceNode = null;
          state.stt.processorNode = null;
          state.stt.sinkNode = null;
          state.stt.flushTimer = null;
          state.stt.pcmFrames = [];
          state.stt.bufferedSamples = 0;
          state.stt.flushInFlight = false;
          state.stt.signalPeak = 0;
          state.stt.gateUntilMs = 0;
          state.stt.lastPartialEmitAtMs = 0;
          state.stt.lastSilenceDropLogAtMs = 0;
          state.stt.lastSegmentDropLogAtMs = 0;
          state.stt.vadState = "idle";
          state.stt.lastVoiceAtMs = 0;
          state.stt.segmentStartedAtMs = 0;
          state.stt.vadStartedAtMs = 0;
          state.stt.vadConfirmed = false;
          state.stt.vadEventActive = false;

          if (flushTimer) {
            clearInterval(flushTimer);
          }

          if (processorNode) {
            try {
              processorNode.onaudioprocess = null;
              processorNode.disconnect();
            } catch (_) {
              // Ignore processor disconnect errors.
            }
          }
          if (sourceNode) {
            try {
              sourceNode.disconnect();
            } catch (_) {
              // Ignore source disconnect errors.
            }
          }
          if (sinkNode) {
            try {
              sinkNode.disconnect();
            } catch (_) {
              // Ignore sink disconnect errors.
            }
          }
          if (audioContext) {
            Promise.resolve(audioContext.close()).catch(() => {});
          }

          if (stream) {
            for (const track of stream.getTracks()) {
              try {
                track.stop();
              } catch (_) {
                // Ignore track stop errors.
              }
            }
          }

          setRecognitionState("stopped");
          return true;
        }

        async function startOpenAiStt(options = {}) {
          if (
            !navigator.mediaDevices?.getUserMedia ||
            !(window.AudioContext || window.webkitAudioContext)
          ) {
            addLog("openai-stt unavailable: WebAudio/getUserMedia is missing.");
            setRecognitionState("unsupported", true);
            return false;
          }

          state.shouldListen = true;
          if (state.stt.active) {
            return true;
          }

          const requestedChunkMs = Number(options.chunkMs);
          const requestedDeviceId = String(options.deviceId || "").trim();
          const requestedDeviceLabel = String(options.deviceLabel || "").trim();
          const preferLoopbackOption = options.preferLoopback;
          const requestedMinSignalPeak = Number(options.minSignalPeak);
          const requestedVadThreshold = Number(options.vadThreshold);
          const requestedHangoverMs = Number(options.hangoverMs);
          const requestedSegmentMinMs = Number(options.segmentMinMs);
          const requestedSegmentMaxMs = Number(options.segmentMaxMs);
          const requestedPartialsEnabled = options.partialsEnabled;
          const requestedPartialEmitMs = Number(options.partialEmitMs);
          const requestedBargeInMinMs = Number(options.bargeInMinMs);

          state.stt.chunkMs = Number.isFinite(requestedChunkMs)
            ? Math.max(120, Math.min(10000, Math.trunc(requestedChunkMs)))
            : state.stt.chunkMs;
          state.stt.deviceId = requestedDeviceId || state.stt.deviceId;
          if (typeof requestedPartialsEnabled === "boolean") {
            state.stt.partialsEnabled = requestedPartialsEnabled;
          }
          if (Number.isFinite(requestedPartialEmitMs)) {
            state.stt.partialEmitMs = Math.max(
              120,
              Math.min(3000, Math.trunc(requestedPartialEmitMs))
            );
          }
          if (Number.isFinite(requestedBargeInMinMs)) {
            state.stt.bargeInMinMs = Math.max(
              80,
              Math.min(5000, Math.trunc(requestedBargeInMinMs))
            );
          }
          if (typeof preferLoopbackOption === "boolean") {
            state.stt.preferLoopback = preferLoopbackOption;
          }
          if (Number.isFinite(requestedMinSignalPeak)) {
            state.stt.minSignalPeak = Math.max(
              0,
              Math.min(1, requestedMinSignalPeak)
            );
          }
          if (Number.isFinite(requestedVadThreshold)) {
            state.stt.vadThreshold = Math.max(
              0,
              Math.min(1, requestedVadThreshold)
            );
          }
          if (Number.isFinite(requestedHangoverMs)) {
            state.stt.hangoverMs = Math.max(
              120,
              Math.min(8000, Math.trunc(requestedHangoverMs))
            );
          }
          if (Number.isFinite(requestedSegmentMinMs)) {
            state.stt.segmentMinMs = Math.max(
              120,
              Math.min(12000, Math.trunc(requestedSegmentMinMs))
            );
          }
          if (Number.isFinite(requestedSegmentMaxMs)) {
            state.stt.segmentMaxMs = Math.max(
              400,
              Math.min(30000, Math.trunc(requestedSegmentMaxMs))
            );
          }
          if (state.stt.segmentMaxMs <= state.stt.segmentMinMs) {
            state.stt.segmentMaxMs = Math.min(
              30000,
              state.stt.segmentMinMs + 800
            );
          }

          try {
            const audioConstraints = {
              echoCancellation: false,
              noiseSuppression: false,
              autoGainControl: false,
              channelCount: 1
            };

            let selectedDeviceId = state.stt.deviceId;
            let selectedDeviceLabel = "";
            if (!selectedDeviceId && requestedDeviceLabel) {
              const devices = await ensureInputLabelsAvailable();
              if (devices.length > 0) {
                addLog(
                  `openai-stt audio inputs: ${devices
                    .map((device, index) => formatDeviceLabel(device, index))
                    .join(" | ")}`
                );
              } else {
                addLog("openai-stt audio inputs: none detected");
              }

              const byLabel = findDeviceByLabel(devices, requestedDeviceLabel);
              if (byLabel?.deviceId) {
                selectedDeviceId = byLabel.deviceId;
                selectedDeviceLabel = byLabel.label || "";
              }
            } else if (!selectedDeviceId) {
              const devices = await ensureInputLabelsAvailable();
              if (devices.length > 0) {
                addLog(
                  `openai-stt audio inputs: ${devices
                    .map((device, index) => formatDeviceLabel(device, index))
                    .join(" | ")}`
                );
              } else {
                addLog("openai-stt audio inputs: none detected");
              }

              if (state.stt.preferLoopback) {
                const loopback = findLoopbackDevice(devices);
                if (loopback?.deviceId) {
                  selectedDeviceId = loopback.deviceId;
                  selectedDeviceLabel = loopback.label || "";
                }
              }
            }

            if (selectedDeviceId && selectedDeviceLabel) {
              addLog(
                `openai-stt selected input by rule: ${selectedDeviceLabel} [${selectedDeviceId}]`
              );
            }

            if (selectedDeviceId) {
              audioConstraints.deviceId = { exact: selectedDeviceId };
            }
            const requestStream = async (constraints) => {
              return navigator.mediaDevices.getUserMedia({
                audio: constraints,
                video: false
              });
            };

            let stream;
            try {
              stream = await requestStream(audioConstraints);
            } catch (err) {
              const errorName = String(err?.name || "");
              const errorMessage = String(err?.message || err || "");
              const isOverconstrained =
                errorName === "OverconstrainedError" ||
                errorName === "ConstraintNotSatisfiedError";

              if (!isOverconstrained) {
                throw err;
              }

              // Fallback #1: keep selected device, relax processing constraints.
              addLog(
                `openai-stt getUserMedia overconstrained; retrying with relaxed constraints (device=${selectedDeviceId || "default"}): ${errorMessage}`
              );
              const relaxedWithDevice = selectedDeviceId
                ? { deviceId: { exact: selectedDeviceId } }
                : true;

              try {
                stream = await requestStream(relaxedWithDevice);
              } catch (relaxedErr) {
                const relaxedErrorName = String(relaxedErr?.name || "");
                const relaxedMessage = String(relaxedErr?.message || relaxedErr || "");
                const relaxedOverconstrained =
                  relaxedErrorName === "OverconstrainedError" ||
                  relaxedErrorName === "ConstraintNotSatisfiedError";

                if (!selectedDeviceId || !relaxedOverconstrained) {
                  throw relaxedErr;
                }

                // Fallback #2: stale/broken device id, fall back to browser default input.
                addLog(
                  `openai-stt selected device is unavailable; falling back to default input: ${relaxedMessage}`
                );
                stream = await requestStream(true);
                selectedDeviceId = "";
                selectedDeviceLabel = "";
              }
            }

            // Cache the effective device id for restarts after speech playback.
            state.stt.deviceId = selectedDeviceId || state.stt.deviceId;

            const AudioContextCtor =
              window.AudioContext || window.webkitAudioContext;
            const audioContext = new AudioContextCtor();
            if (audioContext.state !== "running") {
              await audioContext.resume();
            }
            const sourceNode = audioContext.createMediaStreamSource(stream);
            const processorNode = audioContext.createScriptProcessor(4096, 1, 1);
            const sinkNode = audioContext.createGain();
            sinkNode.gain.value = 0;

            processorNode.onaudioprocess = (event) => {
              if (!state.stt.active) {
                return;
              }
              const now = Date.now();
              if (now < Number(state.stt.gateUntilMs || 0)) {
                if (state.stt.vadState !== "idle") {
                  emitVadStop("gate");
                  resetOpenAiSttSegmentState();
                }
                return;
              }
              const input = event?.inputBuffer?.getChannelData?.(0);
              if (!input || input.length === 0) {
                return;
              }
              let peak = 0;
              for (let index = 0; index < input.length; index += 1) {
                const abs = Math.abs(input[index]);
                if (abs > peak) {
                  peak = abs;
                }
              }
              if (peak > state.stt.signalPeak) {
                state.stt.signalPeak = peak;
              }
              const isVoice = peak >= Number(state.stt.vadThreshold || 0);

              if (isVoice) {
                if (state.stt.vadState === "idle") {
                  state.stt.vadState = "speaking";
                  state.stt.segmentStartedAtMs = now;
                  state.stt.vadStartedAtMs = now;
                  state.stt.vadConfirmed = false;
                  state.stt.lastVoiceAtMs = now;
                  if (!state.stt.vadEventActive) {
                    state.stt.vadEventActive = true;
                    addLog(
                      `openai-stt vad.start (peak=${peak.toFixed(4)}, threshold=${Number(
                        state.stt.vadThreshold || 0
                      ).toFixed(4)})`
                    );
                    emitBridgeEvent({
                      type: "vad.start",
                      peak
                    });
                  }
                } else {
                  state.stt.lastVoiceAtMs = now;
                }
              }

              if (state.stt.vadState === "idle") {
                return;
              }

              const pcmFrame = float32ToInt16Pcm(input);
              state.stt.pcmFrames.push(pcmFrame);
              state.stt.bufferedSamples += pcmFrame.length;

              const segmentDurationMs =
                state.stt.bufferedSamples /
                Math.max(1, Number(state.stt.sampleRate || 48000)) *
                1000;
              const hangoverMs = Math.max(
                120,
                Number(state.stt.hangoverMs || 700)
              );
              const sinceLastVoiceMs = now - Number(state.stt.lastVoiceAtMs || 0);
              const segmentMaxMs = Math.max(
                Number(state.stt.segmentMinMs || 900) + 100,
                Number(state.stt.segmentMaxMs || 7000)
              );
              const vadSpeechMs = state.stt.vadStartedAtMs
                ? now - Number(state.stt.vadStartedAtMs)
                : 0;

              if (
                !state.stt.vadConfirmed &&
                Number(state.stt.vadStartedAtMs || 0) > 0 &&
                vadSpeechMs >= Math.max(80, Number(state.stt.bargeInMinMs || 220))
              ) {
                state.stt.vadConfirmed = true;
                addLog(
                  `openai-stt vad.confirmed (speechMs=${vadSpeechMs}, peak=${peak.toFixed(
                    4
                  )})`
                );
                emitBridgeEvent({
                  type: "vad.confirmed",
                  reason: "barge-in-min",
                  speechMs: vadSpeechMs,
                  peak
                });
              }

              if (state.stt.partialsEnabled) {
                void emitOpenAiSttPartial("interval");
              }

              if (segmentDurationMs >= segmentMaxMs) {
                void flushOpenAiSttSegment("max-duration");
                return;
              }

              if (!isVoice && sinceLastVoiceMs >= hangoverMs) {
                void flushOpenAiSttSegment("hangover");
              }
            };

            sourceNode.connect(processorNode);
            processorNode.connect(sinkNode);
            sinkNode.connect(audioContext.destination);

            state.stt.stream = stream;
            state.stt.audioContext = audioContext;
            state.stt.sourceNode = sourceNode;
            state.stt.processorNode = processorNode;
            state.stt.sinkNode = sinkNode;
            state.stt.sampleRate = Math.max(
              8000,
              Math.min(96000, Math.trunc(Number(audioContext.sampleRate) || 48000))
            );
            state.stt.flushInFlight = false;
            resetOpenAiSttSegmentState();
            state.stt.active = true;

            try {
              const [track] = stream.getAudioTracks();
              const settings = track?.getSettings?.() || {};
              addLog(
                `openai-stt input: ${track?.label || "unknown"} (deviceId=${
                  settings.deviceId || state.stt.deviceId || "default"
                }, sampleRate=${settings.sampleRate || state.stt.sampleRate || "?"})`
              );
            } catch (_) {
              // Ignore diagnostics logging failures.
            }

            setRecognitionState("openai-stt");
            addLog(
              `openai-stt started (mode=vad-segments, sampleRate=${state.stt.sampleRate}, vadThreshold=${Number(
                state.stt.vadThreshold || 0
              ).toFixed(4)}, minPeak=${Number(
                state.stt.minSignalPeak || 0
              ).toFixed(4)}, hangoverMs=${state.stt.hangoverMs}, segmentMinMs=${state.stt.segmentMinMs}, segmentMaxMs=${state.stt.segmentMaxMs}, partials=${state.stt.partialsEnabled}, partialEmitMs=${state.stt.partialEmitMs}, bargeInMinMs=${state.stt.bargeInMinMs})`
            );
            return true;
          } catch (err) {
            addLog(`openai-stt start failed: ${err?.message || err}`);
            setRecognitionState("error: openai-stt", true);
            state.stt.active = false;
            return false;
          }
        }

        async function withSpeechLock(runPlayback) {
          const playback = {
            stopped: false,
            stopHandlers: []
          };

          state.speaking = true;
          const startGateMs = Math.min(120, Math.max(0, Number(state.silenceMs || 0)));
          state.stt.gateUntilMs = Date.now() + startGateMs;
          setSpeechState("speaking");
          state.currentPlayback = playback;
          addLog(`tts playback start (gateMs=${startGateMs})`);
          emitBridgeEvent({
            source: "bridge-tts",
            type: "tts.playback.start",
            gateMs: startGateMs
          });

          try {
            await unlockAudio();
            await runPlayback(playback);
          } finally {
            if (state.currentPlayback === playback) {
              state.currentPlayback = null;
            }
            const endGateMs = playback.stopped
              ? Math.min(80, Math.max(0, Number(state.silenceMs || 0)))
              : Math.max(0, Number(state.silenceMs || 0));
            state.stt.gateUntilMs = Date.now() + endGateMs;
            state.speaking = false;
            setSpeechState(playback.stopped ? "interrupted" : "idle");
            addLog(
              `tts playback end (state=${playback.stopped ? "interrupted" : "completed"}, gateMs=${endGateMs})`
            );
            emitBridgeEvent({
              source: "bridge-tts",
              type: "tts.playback.end",
              gateMs: endGateMs,
              interrupted: playback.stopped
            });
          }
        }

        async function playAudio(payload = {}) {
          const audioBase64 = String(payload.audioBase64 || "").trim();
          const mimeType = String(payload.mimeType || "audio/mpeg").trim();
          const spokenText = String(payload.text || "").trim();
          const stream = Boolean(payload.stream);

          if (!audioBase64) {
            addLog("openai-audio play skipped: empty payload");
            return false;
          }

          if (stream) {
            state.playbackQueue.push({
              audioBase64,
              mimeType,
              spokenText,
              resolve: () => {}
            });
            notifyPlaybackQueueWaiters();
            void ensureStreamingPlaybackLoop();
            return true;
          }

          let playbackResult = { ok: false, errorMessage: "" };
          await withSpeechLock(async (playback) => {
            playbackResult = await playAudioChunkWithPlayback({
              audioBase64,
              mimeType,
              playback
            });
          });
          if (!playbackResult.ok) {
            if (
              playbackResult.errorMessage &&
              playbackResult.errorMessage !== "playback interrupted"
            ) {
              addLog(`openai-audio play failed: ${playbackResult.errorMessage}`);
            }
            return false;
          }

          addLog(`synthesized(openai): ${spokenText || "<audio>"}`);
          return true;
        }

        function configure(options = {}) {
          if (typeof options.language === "string" && options.language) {
            state.language = options.language;
          }
          if (Number.isFinite(options.silenceMs)) {
            state.silenceMs = options.silenceMs;
          }
          if (Number.isFinite(options.turnSilenceMs)) {
            state.turnSilenceMs = Math.max(120, options.turnSilenceMs);
          }
          if (Number.isFinite(options.ttsDuckLevel)) {
            state.ttsDuckLevel = Math.max(0, Math.min(1, options.ttsDuckLevel));
            applyCurrentAudioVolume();
          }
          if (
            typeof options.ttsOutputDeviceId === "string" &&
            options.ttsOutputDeviceId.trim()
          ) {
            state.ttsOutputDeviceId = options.ttsOutputDeviceId.trim();
            state.ttsOutputConfigured = false;
          }
          if (
            typeof options.ttsOutputDeviceLabel === "string" &&
            options.ttsOutputDeviceLabel.trim()
          ) {
            state.ttsOutputDeviceLabel = options.ttsOutputDeviceLabel.trim();
            state.ttsOutputConfigured = false;
          }
        }

        parseQuery();
        configure({
          language: state.language,
          silenceMs: state.silenceMs,
          turnSilenceMs: state.turnSilenceMs,
          ttsDuckLevel: state.ttsDuckLevel,
          ttsOutputDeviceId: state.ttsOutputDeviceId,
          ttsOutputDeviceLabel: state.ttsOutputDeviceLabel
        });

        window.botBridge = {
          startOpenAiStt,
          stopOpenAiStt,
          playAudio,
          stopSpeaking,
          startRealtime,
          stopRealtime,
          realtimeCreateTextTurn,
          realtimeAppendSystemContext,
          realtimeInterrupt,
          setTtsDucking,
          unlockAudio,
          prepareTtsOutput,
          configure
        };

        addLog("bridge loaded");
        setRecognitionState("ready");
      })();
    </script>
  </body>
</html>
